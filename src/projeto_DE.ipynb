{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3200c32",
   "metadata": {},
   "source": [
    "# Projeto - Crédito para Financiamento de Imóveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do módulo de Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72dc29c",
   "metadata": {},
   "source": [
    "### Base de Dados\n",
    "\n",
    "Serão utilizadas bases de dados com informações cadastrais, histórico de crédito e balanços financeiros de diversos clientes. O conjunto de dados está dividido em treino e teste, todos no formato csv. Toda a modelagem, validação e avaliação deve ser feita em cima do conjunto de treino, subdividindo tal base como achar melhor. Existe também a base das variáveis explicativas, para ajudar no desenvolvimento do projeto.\n",
    "\n",
    "[Baixar aqui](https://s3-sa-east-1.amazonaws.com/lcpi/0694c90a-7782-47f7-8bbc-e611d31f9f21.zip)\n",
    "\n",
    "### Contextualização\n",
    "\n",
    "A PyCoders Ltda., cada vez mais especializada no mundo da Inteligência Artificial e Ciência de Dados, foi procurada por uma fintech para desenvolver um projeto de concessão de crédito para imóveis. Nesse projeto, espera-se a criação de score que discrimine ao máximo os bons pagadores dos maus pagadores. Para isso, foi disponibilizada uma base de dados com milhares de casos de empréstimos do passado, com diversas características dos clientes. Deve ser entregue um modelo para realizar essa classificação. Por questões contratuais, o pagamento será realizado baseado no desempenho (ROC AUC).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3311447",
   "metadata": {},
   "source": [
    "### Antes de modelar\n",
    "\n",
    "1. Crie um `fluxo de dados` no Databricks para as bases que serão utiizadas.\n",
    "    1. Insira os dados brutos na primeira camada.\n",
    "    1. Salve as transformações / limpezas na segunda camada.\n",
    "   \n",
    "    1. Crie uma pipeline para o processo.\n",
    "        1. Pipeline, nesse caso, será um único script que gere todas as tabelas acima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f7257",
   "metadata": {},
   "source": [
    "### Durante a modelagem\n",
    "\n",
    "1. Selecione e salve as colunas relevantes e features criadas na terceira camada.\n",
    "\n",
    "1. Salve as versões do modelo no formato `pickle` no DBFS.\n",
    "\n",
    "1. Mantenha o controle das versões criando uma tabela no formato\n",
    "**ATENÇÂO:** Como estamos na versão community, lembre de exportar a tabela abaixo para o DBFS antes da sessão encerrar.\n",
    "\n",
    "|id_modelo|nome_modelo|data_treino|método|roc_auc|tempo_de_treino(s)|hyperparametros|path_to_pickle\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|1|RandomForestRapida|2022-02-22|Random Forest Simples|0.76|124|\\[max_depth=4, ...\\]|/FileStore/tables/modelos/...|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18996267",
   "metadata": {},
   "source": [
    "### Após modelar\n",
    "\n",
    "1. Salve os dados de treino e validação em uma tabela (para que seja possível reproduzir resultados no futuro).\n",
    "    1. Crie uma coluna com o id do modelo escolhido para refêrencia. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f7f04",
   "metadata": {},
   "source": [
    "### Regras de Entrega\n",
    "\n",
    "1. Um notebook (databricks) com a pipeline que gere as tabelas da fase *Antes de modelar*. O notebook deve rodar de uma vez só!\n",
    "\n",
    "1. Um arquivo `.csv` com as informações da tabela gerada na fase *Durante a modelagem*.\n",
    "\n",
    "1. Um notebook que gere as tabelas de treino e validação do passo *Após modelar*. O notebook deve rodar de uma vez só!\n",
    "\n",
    "> **IMPORTANTE:** Tendo em vista que não teremos apresentação do projeto, é indispensável que ele esteja organizado e comentado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos auxiliares para visualização dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Para facilitar o processo de análise dos dados e escolha do tipo de limpeza/transformações que adotaremos, resolvemos criar algumas funções auxiliares para utilizar a [tabela de descrição das colunas](../data/HomeCredit_columns_description.csv) de modo mais interativo e dinânmico. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
